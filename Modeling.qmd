---
title: "ST 558 Project 3: Modeling"
author: "Lamia Benyamine"
date: "July 29, 2024"
format: html
editor: visual
---

# Introduction

# Models

This analysis will create models for predicting the Diabetes variable using the caret package. We will split the data and create multiple models of each logistic regression, classification tree, and a random forest fit using the training set. Then we will use logLoss as the metric and a 5 fold cross-validation to evaluate and select the best fit from each prediction model. Then we will test the final three models using the testing set.

Log loss is a common loss function and is also used as a metric to compare models. Log loss estimates how accurate the prediction probability (p) is to the corresponding actual value (y).

$$
logLoss = -\frac{1}{N} \sum[y_ilnp_i + (1-y_i)ln(1-p_i)]
$$

The higher log loss value, the more the prediction model differs from the actual value. A log loss value is calculated for each observation (i) both actual and prediction, and then a negative average is taken across all observations (N) to get a model log loss value. This metric is better than using accuracy because accuracy is the count of predictions where the predicted value equals the actual value. Whereas log loss takes into account the bias of the prediction based on the variance from the actual label. This gives us a better look into the performance of the model.

## Split the Data

Load libraries necessary for this analysis.

```{r, message = FALSE, warning = FALSE}
library(readr)
library(tidyverse)
library(dplyr)
library(caret)
library(ranger) #Used for a faster implementation of Random Forests 
```

Read in the diabetes data using a relative path, and convert any variables to a factor if needed.

```{r}
diabetes_tb <- read_csv("data/diabetes_binary_health_indicators_BRFSS2015.csv", show_col_types = FALSE) |>
                select(-c(CholCheck, MentHlth, DiffWalk, NoDocbcCost, Education, PhysHlth)) |>
                mutate(across(-c(Diabetes_binary, BMI, GenHlth, Sex, Age, Income), \(x) factor(x, levels = c("0","1"), labels = c("No", "Yes"))),
                       Diabetes = factor(Diabetes_binary, levels = c(0:1), labels = c("no diabetes", "diabetes")),
                       GenHlth = factor(GenHlth, levels = c(1:5), labels = c("excellent", "very good", "good", "fair", "poor")),
                       Sex = factor(Sex, levels = c("0", "1"), labels = c("Female", "Male")),
                       Age = factor(Age, levels = c(1:13), labels=c("18-24", "25-34", "25-34", "35-44", "35-44", "45-54", "45-54", "55-64", "55-64", "65-74", "65-74", "75+", "75+")),
                       Income = factor(Income, levels = c(1:8), labels = c("less than $10k","less than $35k", "less than $35k", "less than $35k", "less than $35k", "less than $75k", "less than $75k", "more than $75k" ))) |>
                select(-Diabetes_binary)
```

Create dummy columns corresponding to the categorical variables for use in our models.

```{r}
newCols <- dummyVars(~ Sex + Age + GenHlth + HighBP + HighChol + Income, data = diabetes_tb) |>
predict(newdata = diabetes_tb)
#add those and remove originals
diabetes_tb <- cbind(diabetes_tb, newCols)

#add '-' to variables that have spaces in their levels in order to run the regression models
levels(diabetes_tb$Diabetes) <- gsub("[^[:alnum:]]", "_", levels(diabetes_tb$Diabetes))
levels(diabetes_tb$Income) <- gsub("[^[:alnum:]]", "_", levels(diabetes_tb$Income))
levels(diabetes_tb$GenHlth) <- gsub("[^[:alnum:]]", "_", levels(diabetes_tb$GenHlth))
```

Split your data into a training and test set with 70:30 ratio.

```{r}
#Set seed to get the same training and test set each time
set.seed(99)
split <- createDataPartition(y = diabetes_tb$Diabetes, p = 0.7, list = FALSE)
head(split)

#Training set receives 70% of data
train <- diabetes_tb[split, ]
#Testing set receives 30% of data
test <- diabetes_tb[-split, ]
```

## Logistic Regression Models

A logistic regression model is used for qualitative response variables and estimates the probability that the response belongs to a particular category. It is best used for a response variable with two classes and can only take on values on the real line which is perfect for binary variables. This data sets response variable is 'Diabetes' which has a binary classification of 0 for no diabetes and 1 for diabetes. Using maximum likelihood estimation, different values of the coefficient are tested through multiple iterations to optimize the best fit of log odds.

We will fit three models using this method and determine the model with the best fit.

```{r}
#Fit all predictor variables
logRegFit1 <- train(Diabetes ~ Sex + Age + GenHlth + HighBP + HighChol + Income + PhysActivity + Fruits + Veggies + Smoker + Stroke + HeartDiseaseorAttack + BMI + HvyAlcoholConsump + AnyHealthcare,
                  data = train,
                  method = "glm",
                  family = "binomial",
                  metric = "logLoss",
                  trControl = trainControl(method = "cv", number = 5,
                                          classProbs = TRUE, summaryFunction = mnLogLoss))
logRegFit1

#Fit main variables based on EDA
logRegFit2 <- train(Diabetes ~ Sex + Age + GenHlth + HighBP + HighChol + Income + BMI,
                  data = train,
                  method = "glm",
                  family = "binomial",
                  metric = "logLoss",
                  trControl = trainControl(method = "cv", number = 5,
                                          classProbs = TRUE, summaryFunction = mnLogLoss))
logRegFit2

#Fit using interactions
logRegFit3 <- train(Diabetes ~ Sex*Age*BMI + GenHlth + HighBP + HighChol + Income,
                  data = train,
                  method = "glm",
                  family = "binomial",
                  metric = "logLoss",
                  trControl = trainControl(method = "cv", number = 5,
                                          classProbs = TRUE, summaryFunction = mnLogLoss))
logRegFit3
```
Based on the logloss metric, the first logical regression fit has the lowest log loss, and thus the best model.

## Classification Tree

A classification tree model is...

We will fit a classification tree with varying values of the complexity parameter and determine the best model.

```{r}
classTreeFit <- train(Diabetes ~ Sex + Age + GenHlth + HighBP + HighChol + Income + PhysActivity + Fruits + Veggies + Smoker + Stroke + HeartDiseaseorAttack + BMI + HvyAlcoholConsump + AnyHealthcare,
                  data = train,
                  method = "rpart",
                  metric = "logLoss",
                  trControl = trainControl(method = "cv", number = 5,
                                          classProbs = TRUE, summaryFunction = mnLogLoss))
classTreeFit
```

Based on the logloss metric and the lowest complexity parameter, the optimal classification tree model has a cp of 0.0008487249.

## Random Forest

A random forest model uses the same idea as bagging and creates multiple trees from bootstrap samples and averages the results. This model is a special case of bagging where m=p. 
There are 11 classification variables so m = 11

We will fit a a random forest model and determine the best model. 

```{r}
m <- c(1:11)
split <- c("extratrees")
min.node <- c(100)
rfGrid <- expand.grid(mtry = m, splitrule = split, min.node.size = min.node)

randForFit <- train(Diabetes ~ Sex + Age + GenHlth + HighBP + HighChol + Income + PhysActivity + Veggies  + Stroke + HeartDiseaseorAttack + BMI,
                  data = train,
                  method = "ranger",
                  metric = "logLoss",
                  num.trees = 100,
                  trControl = trainControl(method = "cv", number = 5,
                                          classProbs = TRUE, summaryFunction = mnLogLoss),
                  tuneGrid = rfGrid)

randForFit
```

Based on the lowest logloss, the best random forest model had mtry = 8.

# Compare models

With the three best models from the above analysis, we will compare them using the test set and determine the best overall model.

```{r}
logPred <- confusionMatrix(data = test$Diabetes, reference = predict(logRegFit1, newdata = test))
classPred <- confusionMatrix(data = test$Diabetes, reference = predict(classTreeFit, newdata = test))
randPred <- confusionMatrix(data = test$Diabetes, reference = predict(randForFit, newdata = test))

logPred <-- mnLogLoss(data = test$Diabetes, reference = predict(logRegFit1, newdata = test))

calcLogLoss<- function(actual, pred)
{
  result = -mean(actual * log(pred) + (1 - actual) * log(1 - pred))
  return(result)
}
calcLogLoss(test$Diabetes, predict(logRegFit1, newdata = test, type="prob") )

logPred <- mnLogLoss(predict(logRegFit1, newdata = test, type = "prob"), lev = c("no_diabetes","diabetes"))

logPred <- mnLogLoss(data.frame(obs = test$Diabetes, 
                                pred =predict(logRegFit1, newdata = test)), 
                     lev = test$Diabetes)


#data.frame(obs = test$Diabetes, pred =predict(logRegFit1, newdata = test)))
classPred <- predict(classTreeFit, newdata = test)
randPred <- predict(randForFit, newdata = test)

#Summary table to determine the best model
fitStats <- data.frame(model = c("Logistic Regression", "Classification Tree", "Random Forest"),
                      )
fitStats
```
